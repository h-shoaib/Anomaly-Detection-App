{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEDDVWF5GGT2",
        "outputId": "579628da-5214-4023-b0b0-4fd09ca97668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity results for pdf4:\n",
            "Similarity with pdf1: 0.54\n",
            "Similarity with pdf2: 0.55\n",
            "Similarity with pdf3: 0.41\n",
            "Likelihood of similar authors: Low for pdf1\n",
            "Likelihood of similar authors: Low for pdf2\n",
            "Likelihood of similar authors: Low for pdf3\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to preprocess text and extract features\n",
        "def extract_features(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text.lower())\n",
        "\n",
        "    # Tokenize sentences\n",
        "    sentences = list(doc.sents)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    # Tokenize words and remove stopwords\n",
        "    words_filtered = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "    # Calculate lexical diversity\n",
        "    lexical_diversity = len(set(words_filtered)) / len(words_filtered) if len(words_filtered) > 0 else 0\n",
        "\n",
        "    # Calculate average sentence length\n",
        "    avg_sentence_length = len(words_filtered) / sentence_count if sentence_count > 0 else 0\n",
        "\n",
        "    # Part-of-speech tagging to get noun, verb, adjective counts\n",
        "    noun_count = sum(1 for token in doc if token.pos_ == 'NOUN')\n",
        "    verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n",
        "    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n",
        "\n",
        "    # Total count of content words\n",
        "    total_content_words = len(words_filtered)\n",
        "\n",
        "    # Calculate percentage of nouns, verbs, and adjectives\n",
        "    noun_percentage = noun_count / total_content_words if total_content_words > 0 else 0\n",
        "    verb_percentage = verb_count / total_content_words if total_content_words > 0 else 0\n",
        "    adj_percentage = adj_count / total_content_words if total_content_words > 0 else 0\n",
        "    other_percentage = 1 - (noun_percentage + verb_percentage + adj_percentage)\n",
        "\n",
        "    # Readability score (using average sentence length and average token length)\n",
        "    syllables_per_word = np.mean([len(token) for token in words_filtered])  # Approximation of syllables per word\n",
        "    readability_score = avg_sentence_length * syllables_per_word\n",
        "\n",
        "    # Return all features in a dictionary\n",
        "    features = {\n",
        "        'readability_score': readability_score,\n",
        "        'lexical_diversity': lexical_diversity,\n",
        "        'sentence_count': sentence_count,\n",
        "        'avg_sentence_length': avg_sentence_length,\n",
        "        'noun_percentage': noun_percentage,\n",
        "        'verb_percentage': verb_percentage,\n",
        "        'adj_percentage': adj_percentage,\n",
        "        'other_percentage': other_percentage\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "# Hardcoded feature data for pdf1, pdf2, pdf3 (same as in the previous step)\n",
        "pdf_features = {\n",
        "    'pdf1': {\n",
        "        'readability_score': 14.8,\n",
        "        'lexical_diversity': 0.72,\n",
        "        'sentence_count': 32,\n",
        "        'avg_sentence_length': 18.4,\n",
        "        'noun_percentage': 0.40,\n",
        "        'verb_percentage': 0.22,\n",
        "        'adj_percentage': 0.16,\n",
        "        'other_percentage': 0.22\n",
        "    },\n",
        "    'pdf2': {\n",
        "        'readability_score': 13.5,\n",
        "        'lexical_diversity': 0.68,\n",
        "        'sentence_count': 28,\n",
        "        'avg_sentence_length': 20.2,\n",
        "        'noun_percentage': 0.35,\n",
        "        'verb_percentage': 0.25,\n",
        "        'adj_percentage': 0.20,\n",
        "        'other_percentage': 0.20\n",
        "    },\n",
        "    'pdf3': {\n",
        "        'readability_score': 12.5,\n",
        "        'lexical_diversity': 0.56,\n",
        "        'sentence_count': 45,\n",
        "        'avg_sentence_length': 15.3,\n",
        "        'noun_percentage': 0.38,\n",
        "        'verb_percentage': 0.30,\n",
        "        'adj_percentage': 0.22,\n",
        "        'other_percentage': 0.10\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to calculate similarity between two PDFs\n",
        "def calculate_similarity(pdf1_features, pdf2_features):\n",
        "    # Extract features into arrays for comparison\n",
        "    features1 = np.array([\n",
        "        pdf1_features['readability_score'],\n",
        "        pdf1_features['lexical_diversity'],\n",
        "        pdf1_features['sentence_count'],\n",
        "        pdf1_features['avg_sentence_length'],\n",
        "        pdf1_features['noun_percentage'],\n",
        "        pdf1_features['verb_percentage'],\n",
        "        pdf1_features['adj_percentage'],\n",
        "        pdf1_features['other_percentage']\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    features2 = np.array([\n",
        "        pdf2_features['readability_score'],\n",
        "        pdf2_features['lexical_diversity'],\n",
        "        pdf2_features['sentence_count'],\n",
        "        pdf2_features['avg_sentence_length'],\n",
        "        pdf2_features['noun_percentage'],\n",
        "        pdf2_features['verb_percentage'],\n",
        "        pdf2_features['adj_percentage'],\n",
        "        pdf2_features['other_percentage']\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(features1, features2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "# Function to compare pdf4 with pdf1, pdf2, and pdf3\n",
        "def compare_with_existing_pdfs(pdf4_text, pdf_features):\n",
        "    # Extract features for pdf4\n",
        "    pdf4_features = extract_features(pdf4_text)\n",
        "\n",
        "    results = {}\n",
        "    for pdf_name, features in pdf_features.items():\n",
        "        similarity_score = calculate_similarity(pdf4_features, features)\n",
        "        results[pdf_name] = similarity_score\n",
        "    return results\n",
        "\n",
        "# Input the raw text for pdf4\n",
        "pdf4_text = \"\"\"\n",
        "    [Your raw text here for pdf4. It could be a large block of text extracted from the PDF file.]\n",
        "\"\"\"\n",
        "\n",
        "# Compare pdf4 with pdf1, pdf2, and pdf3\n",
        "similarity_results = compare_with_existing_pdfs(pdf4_text, pdf_features)\n",
        "\n",
        "# Display the results\n",
        "print(\"Similarity results for pdf4:\")\n",
        "for pdf_name, similarity_score in similarity_results.items():\n",
        "    print(f\"Similarity with {pdf_name}: {similarity_score:.2f}\")\n",
        "\n",
        "# Determine likelihood of similar authors\n",
        "threshold = 0.8  # A threshold to decide if the documents are by the same author\n",
        "for pdf_name, similarity_score in similarity_results.items():\n",
        "    if similarity_score > threshold:\n",
        "        print(f\"Likelihood of similar authors: High for {pdf_name}\")\n",
        "    else:\n",
        "        print(f\"Likelihood of similar authors: Low for {pdf_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***UNIT TESTING***"
      ],
      "metadata": {
        "id": "G4Bp3Tb8HML4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestPDFComparison(unittest.TestCase):\n",
        "\n",
        "    def test_extract_features(self):\n",
        "        sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "        features = extract_features(sample_text)\n",
        "        self.assertIsInstance(features, dict)\n",
        "        self.assertIn('readability_score', features)\n",
        "        self.assertGreater(features['readability_score'], 0)\n",
        "\n",
        "    def test_calculate_similarity(self):\n",
        "        pdf1_features = pdf_features['pdf1']\n",
        "        pdf2_features = pdf_features['pdf2']\n",
        "        similarity = calculate_similarity(pdf1_features, pdf2_features)\n",
        "        self.assertGreaterEqual(similarity, 0)\n",
        "        self.assertLessEqual(similarity, 1)\n",
        "\n",
        "    def test_compare_with_existing_pdfs(self):\n",
        "        pdf4_text = \"This is a sample text for PDF 4.\"\n",
        "        results = compare_with_existing_pdfs(pdf4_text, pdf_features)\n",
        "        self.assertIsInstance(results, dict)\n",
        "        for key, value in results.items():\n",
        "            self.assertIn(key, pdf_features.keys())\n",
        "            self.assertGreaterEqual(value, 0)\n",
        "            self.assertLessEqual(value, 1)\n",
        "\n",
        "# This will run the tests only if the script is executed directly, not when imported\n",
        "def run_tests():\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(TestPDFComparison)\n",
        "    unittest.TextTestRunner().run(suite)\n",
        "\n",
        "run_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGVCZ1avGtAJ",
        "outputId": "9242af38-33a1-4a56-854d-241bbb3285a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.033s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***PERFORMANCE TESTING***"
      ],
      "metadata": {
        "id": "Qy7Auxl1H4lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def test_performance():\n",
        "    pdf4_text = \" \".join([\"This is a test sentence.\"] * 1000)\n",
        "    start_time = time.time()\n",
        "    features = extract_features(pdf4_text)\n",
        "    print(\"Feature extraction time:\", time.time() - start_time)\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = compare_with_existing_pdfs(pdf4_text, pdf_features)\n",
        "    print(\"Comparison time:\", time.time() - start_time)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_performance()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJVRe9ffIB6P",
        "outputId": "c259ef86-518d-4277-c097-bb1769bc7129"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction time: 0.946648120880127\n",
            "Comparison time: 1.3886218070983887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***REGRESSION TESTING***"
      ],
      "metadata": {
        "id": "eD1HbN4TIIB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_regression():\n",
        "\n",
        "    expected_similarity = {\n",
        "        'pdf1': 0.85,\n",
        "        'pdf2': 0.77,\n",
        "        'pdf3': 0.80\n",
        "    }\n",
        "\n",
        "    pdf4_text = \"\"\"\n",
        "        This is a new document that will be compared against the previous ones. The purpose of this comparison is to identify similarities and differences in the writing style, tone, and structure. It contains multiple sentences, each with its own unique structure and content.\n",
        "    \"\"\"\n",
        "\n",
        "    pdf4_features = extract_features(pdf4_text)\n",
        "    print(f\"Extracted features for pdf4: {pdf4_features}\")\n",
        "\n",
        "    pdf1_features = pdf_features['pdf1']\n",
        "    print(f\"Extracted features for pdf1: {pdf1_features}\")\n",
        "\n",
        "    results = compare_with_existing_pdfs(pdf4_text, pdf_features)\n",
        "\n",
        "    for pdf, score in expected_similarity.items():\n",
        "        actual_score = results[pdf]\n",
        "        print(f\"Expected similarity for {pdf}: {score:.2f}, Actual: {actual_score:.2f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_regression()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpiLj8sUIHM-",
        "outputId": "cfd93144-2b30-4577-b676-4ec0995e6ce5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted features for pdf4: {'readability_score': 50.333333333333336, 'lexical_diversity': 0.95, 'sentence_count': 3, 'avg_sentence_length': 6.666666666666667, 'noun_percentage': 0.65, 'verb_percentage': 0.15, 'adj_percentage': 0.25, 'other_percentage': -0.050000000000000044}\n",
            "Extracted features for pdf1: {'readability_score': 14.8, 'lexical_diversity': 0.72, 'sentence_count': 32, 'avg_sentence_length': 18.4, 'noun_percentage': 0.4, 'verb_percentage': 0.22, 'adj_percentage': 0.16, 'other_percentage': 0.22}\n",
            "Expected similarity for pdf1: 0.85, Actual: 0.48\n",
            "Expected similarity for pdf2: 0.77, Actual: 0.48\n",
            "Expected similarity for pdf3: 0.80, Actual: 0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***INTEGRATION TESTING***"
      ],
      "metadata": {
        "id": "Tt68EqBWJRUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_integration():\n",
        "    pdf4_text = \"Integration test text to evaluate the system end-to-end.\"\n",
        "    results = compare_with_existing_pdfs(pdf4_text, pdf_features)\n",
        "    assert isinstance(results, dict), \"Integration test failed: Results should be a dictionary\"\n",
        "    for pdf_name, similarity_score in results.items():\n",
        "        assert 0 <= similarity_score <= 1, f\"Invalid similarity score for {pdf_name}: {similarity_score}\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_integration()"
      ],
      "metadata": {
        "id": "YCuLK7aOJYlS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***USABILITY TESTING***"
      ],
      "metadata": {
        "id": "bm2H8myeJpvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_usability():\n",
        "    # Test 1: Empty input\n",
        "    empty_text = \"\"\n",
        "    empty_features = extract_features(empty_text)\n",
        "    print(\"Features for empty input:\", empty_features)\n",
        "\n",
        "    # Test 2: Normal input\n",
        "    normal_text = \"\"\"\n",
        "    This is a test document. It contains several sentences with different word structures and parts of speech.\n",
        "    The purpose of this document is to check the functionality of the feature extraction.\n",
        "    \"\"\"\n",
        "    normal_features = extract_features(normal_text)\n",
        "    print(\"Features for normal input:\", normal_features)\n",
        "\n",
        "    # Test 3: Similarity calculation on extracted features\n",
        "    pdf_features = {\n",
        "        'pdf1': {\n",
        "            'readability_score': 14.8,\n",
        "            'lexical_diversity': 0.72,\n",
        "            'sentence_count': 32,\n",
        "            'avg_sentence_length': 18.4,\n",
        "            'noun_percentage': 0.40,\n",
        "            'verb_percentage': 0.22,\n",
        "            'adj_percentage': 0.16,\n",
        "            'other_percentage': 0.22\n",
        "        }\n",
        "    }\n",
        "    similarity_result = compare_with_existing_pdfs(normal_text, pdf_features)\n",
        "    print(\"Similarity results for normal input:\", similarity_result)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_usability()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YCk544JJtaS",
        "outputId": "4beca4be-c17c-4bd3-de14-3da14cc2a61e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features for empty input: {'readability_score': nan, 'lexical_diversity': 0, 'sentence_count': 0, 'avg_sentence_length': 0, 'noun_percentage': 0, 'verb_percentage': 0, 'adj_percentage': 0, 'other_percentage': 1}\n",
            "Features for normal input: {'readability_score': 37.666666666666664, 'lexical_diversity': 0.9333333333333333, 'sentence_count': 3, 'avg_sentence_length': 5.0, 'noun_percentage': 0.8, 'verb_percentage': 0.13333333333333333, 'adj_percentage': 0.13333333333333333, 'other_percentage': -0.06666666666666665}\n",
            "Similarity results for normal input: {'pdf1': 0.49208162344561046}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    }
  ]
}